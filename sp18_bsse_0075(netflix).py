# -*- coding: utf-8 -*-
"""SP18-BSSE-0075(NETFLIX).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rwZlJVeKrrJgqVAb3dd5nHfLV4SFVpfa

#<font color="red">NETFLIX DATA ANALYSIS </font>
 ------------------------------
 <font color="crimson">**Name : Muhammad Umair Ali (ID: SP18-BSSE-0075)**</font>

  <font color="crimson">**Name : Zaigham Gulzar (ID: SP18-BSSE-0026)**

  **Section : AM** </font>

  ------------------------------------------------


  <font color="orange">=> Dataset Resource : https://www.kaggle.com/shivamb/netflix-shows/tasks?taskId=116</font>

#<font color="darkviolet">1. Insights </font>

**(Data insights are the results of one's analysis of sets of data relevant to a specific issue or scenario. The analysis of this data gives insights that assist one in making educated decisions while also reducing the risk associated with traditional trial-and-error testing methods.)**

##<font color="pink">1.1. Identifying the Valuable Insights from the dataset.</font>

1. Total no. of Movies & TV Shows on NETFLIX.
2. Amount of content on NETFLIX according to Ratings.
3. Overall content produced on NETFLIX between (2006-2021).
4. Yearly Movies production on NETFLIX from start till now.
5. Yearly TV Show production on NETFLIX from start till now.
6. Yearly Total Content production on NETFLIX from start till now.
7. Top 20 content producing countires on NETFLIX.
8. Top 20 Genres on NETFLIX.
9. Top 20 Directors on NETFLIX.
10. Top 20 Actors on NETFLIX.
11. Count of Movies according to Genre.
12. Count of TV Shows according to Genre.
13. Total Movies uploaded on NETFLIX according to ratings.
14. Total TV Shows uploaded on NETFLIX according to ratings.
15. Number of different Genres present on NETFLIX.
16. A Year with Most produced content on NETFLIX.
17. A Year with Least produced content on NETFLIX.
18. Popular/Flop genre among any country.
19. Average shows/movies released per year.
20. Longest/Shortest duration show/movie on NETFLIX till 2021.
21. Country with most/least shows/movies.
22. Year with maximum/minimum shows/movies released.
23. Are there more movies or shows on NETFLIX.

#<font color="lawngreen">2. Getting Raw Data (Descriptive Analytics)</font>

**(Raw data (sometimes called source data, atomic data or primary data) is data that has not been processed for use. A distinction is sometimes made between data and information to the effect that information is the end product of data processing.)**

##<font color="palegreen">2.1. Including the dependencies.</font>
"""

from google.colab import files
import pandas as pd
import io
import numpy as np

# Uploading the dataset.
data = files.upload()

"""##<font color="palegreen">2.2. Including the dataset source and applying descriptive analytics.</font>"""

# Storing dataset into pandas dataframe
ds = pd.read_csv(io.BytesIO(data["netflix_titles.csv"]))

# Renaming columns.
ds.columns = ["ID", "Type", "Title", "Director", "Cast", "Country", "Date_Added",
              "Release_Year", "Rating", "Duration", "Genre", "Description"]
ds

# Applying Descriptive Analytics to investigate our dataset.
ds.describe()

#Checking no. of rows and columns in our dataset.
ds.shape

# Checking features type.
ds.dtypes

"""#<font color="dodgerblue">3. Cleaning the Data</font>

**(Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. When combining multiple data sources, there are many opportunities for data to be duplicated or mislabeled.)**

##<font color="aqua">3.1. Generating summary report of missing elements.</font>
"""

# Auditing the missing elements and entitites in our dataset.
ds.isnull().sum()

"""##<font color="aqua">3.2. Cleaning Process.</font>"""

# We already have Release_Year so no need of Date_Added for our analysis.
#ds.drop(["Director", "Cast", "Date_Added"], axis = 1, inplace = True)
ds.drop(["Date_Added"], axis = 1, inplace = True)
# To fill the NaN entries in Director, we'll go for Anonymous as an entry.
ds["Director"].fillna("Anonymous", inplace = True)
#same for cast.
ds["Cast"].fillna("Anonymous", inplace = True)

# As NETFLIX is a US based company, so we can replaced NaN Countries with United States.
ds["Country"].replace(np.nan, "United States", inplace = True)

# Now we left with ratings, so we gonna replace NaN with most usual/common rating i.e. TV-MA
ds["Rating"].replace(np.nan, "TV-MA", inplace = True)

#Titles with inappropiate formats. we cant remove # as some of the movies/shows contains # in their names.
ds["Title"].replace("â€‹", "")

#checking the results by displaying first 50 rows and last 50 rows of dataset.
ds.head(50)

ds.tail(50)

"""##<font color='aqua'>3.3. Generating Summary Report inorder to check either all NaNs are filled or not.</font>"""

ds.isnull().any()

"""# <font color="gold">4. Exploratory Data Analysis (EDA)</font>

**(exploratory data analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods.)**

##<font color='olive'>4.1. Analyzing the Data.</font>
"""

# Including dependencies for ploting the graphs.
import matplotlib.pyplot as plt

# Getting information.
ds.info()

# Count of R-rated movies on NETFLIX released.
mov = ds[ds["Type"] == "Movie"]
rmov = mov[mov["Rating"] == "R"]
rmov.count()

# Count of R-rated shows on NETFLIX released.
shw = ds[ds["Type"] == "TV Show"]
rshw = shw[shw["Rating"] == "R"]
rshw.count()

# Count of TV-MA-rated shows on NETFLIX released.
shw = ds[ds["Type"] == "TV Show"]
rshw = shw[shw["Rating"] == "TV-MA"]
rshw.count()

# Count of TV-MA-rated movies on NETFLIX released.
shw = ds[ds["Type"] == "Movie"]
rshw = shw[shw["Rating"] == "TV-MA"]
rshw.count()

#Highest content producing year
yr = ds["Release_Year"].value_counts().sort_values()
yr[-1:]

#Lowest content producing year
yr[:1]

# Average content produced according to years.
avg_con = ds["Release_Year"].value_counts().values.mean()
print("Average Content Produced: ", int(avg_con))

# Average movies produced according to years.
avg_mov = ds[ds['Type'] == 'Movie'].value_counts().count().mean()
print("Average Movie Produced: ", int(avg_mov))

# Average tv shows produced according to years.
avg_tv = ds[ds['Type'] == 'TV Show'].value_counts().count().mean()
print("Average TV Show Produced: ", int(avg_tv))

# let find out hit/ flop genres in the country US
# splitting the genres for the accuracy and precise result.
cnt = ds[ds['Country'] == 'United States'].Genre.str.split(', ', expand=True).stack().reset_index(level=1, drop=True).value_counts()
# hit genre
cnt.iloc[:1]

# flop genre
cnt.iloc[-1:]

# Are there more movies or tv shows on NETFLIX.
me = ds[ds['Type'] == 'Movie'].value_counts().count()
tv = ds[ds['Type'] == 'TV Show'].value_counts().count()

if me > tv:
  print("Movies are more")
else:
  print("Tv Shows are more")

"""##<font color='olive'>4.2. Ploting the analysis into different graphs.</font>"""

# Sorting the data according to year so that we can limit the years to show in the graph.
mtp = ds.sort_values(by='Release_Year')[-7001:]
# Dividing the the Type into Movie & TV Show.
mtp_m = mtp[mtp["Type"] == "Movie"]
mtp_t = mtp[mtp["Type"] == "TV Show"]

# #Lets first visulaize amount of movies and tv shows on NETFLIX uptill now by using pie chart
#getting the information
typ = ds.Type.value_counts()

#plotting the pie chart
plt.figure(figsize=(10,10))
plt.title('NETFLIX Content Classification\n\n', fontsize=15)
plt.pie(typ.values, labels=typ.index, explode=(0.025,0.025), colors=['yellow', 'orange'], startangle=0, autopct='%1.1f%%')
plt.axis('equal')
plt.legend(loc='upper right')
plt.show()

#Ploting the Donut chart of total no of movies & shows according to ratings is present on NETFLIX.
plt.figure(figsize=(20,10))
plt.title('NETFLIX Ratings Ratio\n\n', fontsize=14)
plt.pie(ds.Rating.value_counts(), labels=ds.Rating.value_counts().index, startangle=0, autopct='%1.1f%%')
plt.axis('equal')
#adding a circle in center to make it looks like donut.
my_circle=plt.Circle((0,0), 0.7, color='white')
p=plt.gcf()
p.gca().add_artist(my_circle)
plt.show()

# Getting the data.
cont = mtp.Release_Year.value_counts().sort_index()

# Ploting the line graph with all of the types according to years
# How many content released in different years
plt.figure(figsize=(15,10))
plt.plot(cont.index, cont.values, label='Content', color='red', marker='o')
plt.title('NETFLIX Content (2006-2021)', fontsize=14)
plt.xlabel('Year', fontsize=14)
plt.ylabel('Content', fontsize=14)
plt.legend(loc='upper right')
plt.grid(True)
#displaying respective (x,y) coordinates to the points.
for i_x, i_y in zip(mtp.Release_Year.value_counts().index, mtp.Release_Year.value_counts().values):
    plt.text(i_x, i_y, '({}, {})'.format(i_x, i_y))
plt.show()
#plt.text(2020, 1000, "Red - Movie & TV Show")
#plt.bar(mtp.Release_Year.value_counts().index, mtp.Release_Year.value_counts().values, width=0.5, color='r')

# Gathering the info.
#sorting the the data according to years ascendingly.
mvei = mtp_m.Release_Year.value_counts().sort_index()
tvi = mtp_t.Release_Year.value_counts().sort_index()

toti = mtp.Release_Year.value_counts().sort_index()

# Ploting the bar graph with differentiation of types according to years
# How many Movies & TV Show released in different years
plt.figure(figsize=(10,10))
plt.title('NETFLIX Movies & TV Shows (2006-2021)', fontsize=14)
plt.xlabel('Year', fontsize=14)
plt.ylabel('Movies & TV Shows', fontsize=14)
plt.plot(mvei.index, mvei.values, label='Movie', color='red')
plt.plot(tvi.index, tvi.values, label='TV Show', color='black')
plt.plot(toti.index, toti.values, label='Total', color='grey')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()

# Getting the information.
cs = ds.Country.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)
cty = cs.value_counts()[:20]
clr = ['red', 'purple', 'magenta', 'pink', 'orange','yellow', 'green', 'blue', 'brown', 'black',
       'grey', 'gold', 'turquoise', 'olive', 'maroon', 'teal', 'crimson', 'skyblue', 'violet', 'peru']

# Plotting the Top 10 countries who have produced maximum content on NETFLIX.
# For this insight, we gonna be using bar graph to visualize our analyzation.
plt.figure(figsize=(20,10))
plt.title("Top 20 Content Producing Countries on NETFLIX", fontsize=20)
plt.xlabel("Content", fontsize=20)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.ylabel("Country", fontsize=20)
plt.barh(cty.index, cty.values, color=clr, align='center')
plt.show()

# Now we gonna be visualizing the top 20 genres on NETFLIX in terms of content i.e Movies & TV Show
#firstly, filtering the data so for that we first need to split the elements because we got different genres for a single movie.
gendiv = ds.set_index('Title').Genre.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)
gendiv2 = gendiv.value_counts()[:20]
# Plotting the bar graph
plt.figure(figsize=(15,10))
plt.barh(gendiv2.index, gendiv2.values, color=clr, label='Content')
plt.title('NETFLIX Top 20 Genres', fontsize=14)
plt.xlabel('Content', fontsize=14)
plt.ylabel('Genre', fontsize=14)
plt.show()

# Lets visualize top 20 directors of the content added on NETFLIX.
#Filtering
dirdiv = ds[ds['Director'] != 'Anonymous'].Director.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)
dir = dirdiv.value_counts()[:20]

# Plotting the bar graph of above insight.
plt.figure(figsize=(15,10))
plt.barh(dir.index, dir.values, color=clr, label='Content')
plt.title('NETFLIX Top 20 Directors', fontsize=14)
plt.xlabel('Content', fontsize=14)
plt.ylabel('Director', fontsize=14)
plt.show()

# Lets visualize top 20 Actors of the content added on NETFLIX.
#Filtering
act_div = ds[ds["Cast"] != "Anonymous"].Cast.str.split(", ", expand=True).stack().reset_index(level=1, drop=True)
acts = act_div.value_counts()[:20]

#plotting the insight into bar graph
plt.figure(figsize=(10,10))
plt.barh(acts.index, acts.values, color=clr, label='Content')
plt.title('NETFLIX Top 20 Actors', fontsize=14)
plt.xlabel('Content', fontsize=14)
plt.ylabel('Actors', fontsize=14)
plt.show()

#Now we gonna be visulaizing the amount of content(moive & tv show) released on NETFLIX according to ratings.
#firstly, filtering
rtm = ds[ds['Type'] == 'Movie'].Rating.value_counts()
rtv = ds[ds['Type'] == 'TV Show'].Rating.value_counts()

#Plotting the scatter plot as visualization of the insight
plt.figure(figsize=(17,8))
plt.title('NETFLIX Movies & TV Shows According to Ratings', fontsize=14)
plt.xlabel('Rating', fontsize=14)
plt.ylabel('Amount', fontsize=14)
plt.scatter(rtm.index, rtm.values, label='Movie',color='red')
plt.scatter(rtv.index, rtv.values, label='TV Show',color='black')
plt.grid(True)
plt.legend(loc='upper right')
plt.show()

"""# <font color="red">5. Design the Model</font>

**(the goal is to build the models to do prediction, classification, clustering on given data. It involves Machine Learning, Data Mining and Statistical Techniques.)**

##<font color="tomato">5.1 Decision Tree Model</font>

**(Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter. The tree can be explained by two entities, namely decision nodes and leaves)**

<font color="orange">=> Learning/Training Phase</font>
"""

#Including the dependencies
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.preprocessing import OrdinalEncoder
import pydotplus as pdp
from IPython.display import Image

# Viewing the dataset features
ds.columns

# Now in order to start with decision tree we first need to do some feature engineering on our dataset to be able to use in the model
# By using ordinal encoder we can convert our str data into numeric indentification which makes it eaiser for the model to train and predict.
# Our target is to design a decision tree model over NETFLIX where the model will predict the movie title based upon the information
# gathered from viewer i.e (Type, Country, Release_Year, Rating, Genre).
oe = OrdinalEncoder()

# defining col names for the ordinal dataset 
cols = ['Type', 'Country', 'Rating', 'Genre', 'Title']

# storing values from ds cols.
ty_arr = ds[['Type']].values
cty_arr = ds[['Country']].values
rt_arr = ds[['Rating']].values
gn_arr = ds[['Genre']].values
titl_arr = ds[["Title"]].values

# Transforming the data by using ordinal encoder.
ty_rlt = oe.fit_transform(ty_arr)
cty_rlt = oe.fit_transform(cty_arr)
rt_rlt = oe.fit_transform(rt_arr)
gn_rlt = oe.fit_transform(gn_arr)
titl_rlt = oe.fit_transform(titl_arr)

# Flattening 2D arr into 1D for storing it into ord_ds.
ty_rlt_flt = ty_rlt.flatten()
cty_rlt_flt = cty_rlt.flatten()
rt_rlt_flt = rt_rlt.flatten()
gn_rlt_flt = gn_rlt.flatten()
titl_rlt_flt = titl_rlt.flatten()

#Creating the new dataset and add the ordinal data and target col to the dataset.
ord_ds = pd.DataFrame(columns=cols)
ord_ds['Type'] = ty_rlt_flt
ord_ds['Country'] = cty_rlt_flt
ord_ds['Rating'] = rt_rlt_flt
ord_ds['Genre'] = gn_rlt_flt
ord_ds['Title'] = titl_rlt_flt

# Viewing the results
ord_ds

ds

# Now we gonna start designing the descision tree model.
# firstly, initializing the decision tree classifier
dt_clf = tree.DecisionTreeClassifier()

#get all the records against first four features.
X = ord_ds.iloc[:, :4].values
y = ord_ds['Title'].values

# making testing and training subsets.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Training our decision tree with the training data that we have gathered from one hot encoding
# and giving a predicted answer column i.e. Title by which it can predict.
dt_clf_train = dt_clf.fit(X_train, y_train)

# displaying the text representation of the DT model.
dt_txt = tree.export_text(dt_clf_train)
print(dt_txt)

dot_dt = tree.export_graphviz(dt_clf_train, out_file=None, filled=True, rounded=True,
                              feature_names = None, class_names=None)
graph = pdp.graph_from_dot_data(dot_dt)  
Image(graph.create_png())

"""<font color='orange'>=> Testing/Prediction Phase</font>"""

# predicting our model
prediction = dt_clf_train.predict([[0.0, 429.0, 8.0, 310.0]])
print(prediction[0])

#The title of the content that has been predicted.
ord_dsr_index = ord_ds[ord_ds["Title"] == prediction[0]].index
val = ds[ds['Title'].index == ord_dsr_index.values[0]]
val

# The predicted title in ordinal
ord_ds[ord_ds['Title'] == prediction[0]]

"""##<font color="tomato">5.2 Simple Linear Regression Model</font>

**(n statistics, linear regression is a linear approach to modelling the relationship between a scalar response and one or more explanatory variables. The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression.)**

<font color='orange'>=> Training/Learning Phase</font>
"""

# getting the dependencies
from sklearn.linear_model import LinearRegression

# In order to start with linear regression we first identifies our case 
# Our motive/goal is to predict the amount of content production on NETFLIX in those years that are not really present for example(1948-1953 & 1961 etc.) (for now).
# so firstly we will visualize the content production on NETFLIX up till now i.e 2021.
cpd = ds.Release_Year.value_counts().sort_index()
cpd_yr = cpd.index
cpd_cnt = cpd.values

# Step # 01: Visualizing our overall NETFLIX content production till 2021.
plt.figure(figsize=(45,15))
plt.title('NETFLIX Content Production (1925-2021)', fontsize=14)
plt.xlabel('Year', fontsize=14)
plt.ylabel('Content', fontsize=14)
plt.xticks(cpd_yr.values)
plt.scatter(cpd_yr, cpd_cnt, label='Content', color='darkslategrey')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()

# Now, the task is to find a line which fits best in above scatter plot so that we can predict the response for any new feature values.
# the best fit line is called the regression line.
# for that we will be using simple linear regression equation i.e. y = mx+b where, y = content & x = years
# purpose of using simple linear regression is because our case hase single dependent variable i.e. total content.
# Adding our independent & dependent vars in dataframe.

#Our x i.e. our feature vector or independent var (years).
year_ds = pd.DataFrame(cpd.index, columns=['Year'])

#Our y i.e. response vector or dependent var (content production).
cont_ds = pd.DataFrame(cpd.values, columns=['Content'])

#splitting into test & train subsets.
X_train, X_test, y_train, y_test = train_test_split(year_ds, cont_ds, test_size=0.25, random_state=1)

# Checking the shapes of the subsets
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#Trainging the linear regression model by using available data points.
reg = LinearRegression().fit(X_train, y_train)

#displaying the intercept
print(reg.intercept_)

#displaying the coefficient
reg.coef_

# Lets test our model on the basis of X_test values, How well it can predict the amount of content produced according to the regression line or the best fit line.
X_test

# here our model will predict the optimum content that can be produced in accordance with the regression line.
prd = reg.predict(X_test)
prd

# time to visualize our rgression line or best fit line through scatter & line plot.
plt.figure(figsize=(45,15))
plt.title('NETFLIX Content Production by Regression', fontsize=14)
plt.xlabel('Year', fontsize=14)
plt.ylabel('Content', fontsize=14)
plt.xticks(cpd.index.values)

# plotting the actual data points that we used to test as  a scatter plot 
plt.scatter(X_test, y_test, color = "darkslategrey", label='Actual', marker = "o", s = 30)

# plotting the regression line
plt.plot(X_test, prd, label='Best Fit', color = "crimson")

# function to show plot
plt.legend(loc='upper right')
plt.grid(True)
plt.show()

"""<font color='orange'> => Evaluation of Linear Regression </font>"""

#evaluation of linear regression
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, prd))
print('Mean Square Error :', metrics.mean_squared_error(y_test, prd))
print('Root Mean Square Error:', np.sqrt(metrics.mean_squared_error(y_test, prd)))

"""##<font color='tomato'> 5.3 kNN Classification Model</font>

**(In statistics, the k-nearest neighbors algorithm is a non-parametric classification method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in data set.)**

<font color='orange'>=> Initiation of Model</font>
"""

#Including the dependencies
from sklearn.neighbors import KNeighborsClassifier

#In order to start with kNN classifier, we first need to determine our case that should be handled by kNN model.
# so our scenario is to findout that which content belongs to which type class (R, PG-13 etc.) according to the feactures provided.
# firstly, creating a data set for our scanario.
cols = ['Title', 'Genre', 'Rating', 'Type']
mt_rat = pd.DataFrame(columns=cols)

#Adding the data
# using pd.Series() because if there are some records where there is Nan value, it should add that regardless of that.
mt_rat['Title'] = pd.Series(ord_ds.Title.values)
mt_rat['Genre'] = pd.Series(ord_ds.Genre.values)
mt_rat['Rating'] = pd.Series(ord_ds.Rating.values)
mt_rat['Type'] = pd.Series(ds.Type.values)

mt_rat_sorted = mt_rat.sort_values(by='Type')

mt_rat_sorted

# to start with the dataset we first need to divide our dataset into data features & target features
# X = data features from first col till n-1
X = mt_rat_sorted.iloc[:,:-1]
# y = target feature last col of our dataset.
y = mt_rat_sorted.iloc[:, 3]

"""<font color='orange'>=> Learning/Predicting Phase of Model</font>"""

# now lets divide our dataset into testing & training subsets.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# lets initialize our kNN classifier 
kn = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)

# before prediction, lets analyze our data the we going to use for prediction
X_test

# make some prediction upon testing data.
k_prd = kn.predict(X_test)
k_prd

y_test.values

"""<font color='orange'>=> Evaluation of Model</font>"""

# Evaluation of the model.
print(metrics.classification_report(y_test, k_prd))
print(metrics.confusion_matrix(y_test, k_prd))

# lets try to visualize our results in the form of multiple bars graph.
plt.figure(figsize=(14,10))
plt.title('kNN Type Classification Model', fontsize=14)
plt.xlabel('Type', fontsize=14)
plt.ylabel('Amount', fontsize=14)
plt.bar(y_test, X_test.Title.value_counts().values, label='Actual', width=0.3, color='crimson', align='edge')
plt.bar(k_prd, X_test.Title.value_counts().values, label='Predicted', width=0.15, color='maroon', align='edge')
plt.legend(loc='upper right')
plt.show()